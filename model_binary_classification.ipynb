{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Book 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/HHD_2TB/baurai/aditya_vm/bin/python3.6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used =  3.6.9 (default, Oct  9 2020, 08:58:16) \n",
      "[GCC 6.3.0 20170516]\n",
      "Numpy version =  1.19.5\n",
      "Tensorflow version =  2.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Python version used = \", sys.version)\n",
    "print(\"Numpy version = \", np.__version__)\n",
    "print(\"Tensorflow version = \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"IMAGE_SIZE\" : (512, 512),\n",
    "    \"AUTOTUNE\" : tf.data.experimental.AUTOTUNE,\n",
    "    \"BATCH_SIZE\" : 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training TFRec files =  94\n",
      "Number of validation TFRec files =  21\n",
      "Number of testing TFRec files =  21\n"
     ]
    }
   ],
   "source": [
    "training_filenames = tf.io.gfile.glob(\"tfrec_binary_classification/train*.tfrec\")\n",
    "validation_filenames = tf.io.gfile.glob(\"tfrec_binary_classification/val*.tfrec\")\n",
    "testing_filenames = tf.io.gfile.glob(\"tfrec_binary_classification/test*.tfrec\")\n",
    "\n",
    "print(\"Number of training TFRec files = \", len(training_filenames))\n",
    "print(\"Number of validation TFRec files = \", len(validation_filenames))\n",
    "print(\"Number of testing TFRec files = \", len(testing_filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define The Loading Pipeline : \n",
    "\n",
    "* Call **load_dataset** function : This function is called with validation/training/testing filenames. In case the dataset is unlabeled, simply pass False parameter to it, along with a choice whether you want to load things in order or not. Normally, **we disable it for boosting our computation speed**. This load_training dataset will return our dataset(a pair of **(image, label) if labeled = True** was passed, otherwise **(image, image_name))**. How it does that, will be seen ahead in the notebook.\n",
    "* Now, we have images wih us, in the dataset. We will define and call image_augmentation method to apply some transformation to these images, before we use them further.\n",
    "* Once it is done, we call the repeat function which repeats this dataset so each original value is seen count times. The default behavior (if count is None or -1) is for the dataset be repeated indefinitely.\n",
    "* Shuffling - The shuffle( ) function randomly shuffles the elements of this dataset.Internally, the dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. *For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer*.\n",
    "* Group the images into batches.\n",
    "* **Prefetch( )** : Most dataset input pipelines should end with a call to prefetch. **This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements**.\n",
    "* **cache( )** : The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data. When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset() : \n",
    "    dataset = load_dataset(training_filenames, labeled = True, ordered = False)\n",
    "    dataset = dataset.map(image_augmentation, num_parallel_calls = config[\"AUTOTUNE\"])\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(config[\"BATCH_SIZE\"])\n",
    "    dataset = dataset.prefetch(config[\"AUTOTUNE\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_dataset() :\n",
    "    dataset = load_dataset(validation_filenames, labeled = True, ordered = True)\n",
    "    dataset = dataset.batch(config[\"BATCH_SIZE\"])\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(config[\"AUTOTUNE\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_dataset() : \n",
    "    dataset = load_dataset(testing_filenames, labeled = True, ordered = True)\n",
    "    dataset = dataset.batch(config[\"BATCH_SIZE\"])\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(config[\"AUTOTUNE\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Functions :\n",
    "These functions will help in overall structuring of the pipeline.\n",
    "\n",
    "Setting default value of :\n",
    "\n",
    "* **ordered as False** - For high performance we will be reading data from multiple file sources at once, and the dataset will be shuffled before reading commences.\n",
    "\n",
    "* **labeled as true** - Can set it to False too, doesn't matter much. Or, simply leave it be.\n",
    "\n",
    "*Role of partial( ) functools :*\n",
    "\n",
    "Treat this as an operation which results in a object(called partial object). **These objects when invoked, behave like functions with positional/additional(but not mandatory) arguments**.\n",
    "\n",
    "So, sometimes we may have labels, hence the function call will be initated using labeled = True. Sometimes, we won't. In that case, function calls will be initiated with a False initialization of the labeled argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, labeled = True, ordered = False):\n",
    "    ignore_order = tf.data.Options()\n",
    "    if ordered == False : \n",
    "        ignore_order.experimental_deterministic = False # disable order\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = config[\"AUTOTUNE\"])\n",
    "    dataset = dataset.map(partial(read_tfrecord, labeled = labeled), num_parallel_calls = config[\"AUTOTUNE\"])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to feature mapping present in our TFRecord files, we design the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example, labeled) : \n",
    "    if labeled == True : \n",
    "        tfrecord_format = {\n",
    "            \"image\" : tf.io.FixedLenFeature([]. tf.string),\n",
    "            \"label\" : tf.io.FixedLenFeature([]. tf.int64)\n",
    "        }\n",
    "    else:\n",
    "        tfrecord_format = {\n",
    "            \"image\" : tf.io.FixedLenFeature([], tf.string),\n",
    "            \"image_name\" : tf.io.FixedLenFeature([]. tf.string)\n",
    "        }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    image = decode_image(example[\"image\"])\n",
    "    \n",
    "    if labeled == True : \n",
    "        label = tf.cast(example[\"label\"], tf.int32)\n",
    "        return image, label\n",
    "    else:\n",
    "        image_name = example[\"image_name\"]\n",
    "        return image, image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image) : \n",
    "    image = tf.image.decode_jpeg(image, channels = 3) # decode JPEG-encoded image to uint8 tensor\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    image = image/255.0 \n",
    "    image = tf.reshape(image, [config[\"IMAGE_SIZE\"][0], config[\"IMAGE_SIZE\"][1], 3]) # reshapes tensor structure\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(image, label) : \n",
    "    image = tf.image.resize(image, [config[]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
