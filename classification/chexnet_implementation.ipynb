{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extra-curtis",
   "metadata": {},
   "source": [
    "# ChexNet\n",
    "\n",
    "ChexNet is a 121-layer dense convolutional neural network that inputs a chest X-ray image and outputs the probability of thoracic abnormality(*originally trained for detecting penumonia*), along with a heatmap localizing the areas of the image most indicative of that abnormality.\n",
    "\n",
    "DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "local-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/HHD_2TB/baurai/aditya_pytorch_vm/bin/python3.6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "southwest-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used =  3.6.9 (default, Oct  9 2020, 08:58:16) \n",
      "[GCC 6.3.0 20170516]\n",
      "OpenCV version =  4.5.1\n",
      "PyTorch used =  1.7.0+cu101\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "print(\"Python version used = \", sys.version)\n",
    "print(\"OpenCV version = \", cv2.__version__)\n",
    "print(\"PyTorch used = \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-orchestra",
   "metadata": {},
   "source": [
    "# Dataset : \n",
    "\n",
    "NIH Dataset images available at : \"/media/HHD2/NIH/tflow_obj_detection/images\"\n",
    "\n",
    "Metadata for NIH is available at : \"/media/HHD2/NIH/tflow_obj_detection/data/Data_Entry_2017_v2020.csv\"\n",
    "\n",
    "FILTERED_METADATA_PATH_NIH : contains the OG csv where nodule cases are marked as 1, and healthy as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ranging-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH_NIH = \"/media/HHD2/NIH/tflow_obj_detection/images/\"\n",
    "METADATA_PATH_NIH = \"/media/HHD2/NIH/tflow_obj_detection/data/Data_Entry_2017_v2020.csv\"\n",
    "FILTERED_METADATA_PATH_NIH = \"/media/HHD_2TB/baurai/filtered_metadata/metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-anderson",
   "metadata": {},
   "source": [
    "1 - Nodule\n",
    "0 - Healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "processed-picnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataframe = (66692, 5)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.read_csv(FILTERED_METADATA_PATH_NIH))\n",
    "\n",
    "print(f\"Shape of the dataframe = {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interracial-shanghai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000004_000.png</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000005_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000005_001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000005_002.png</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_id  label  patient_id  age gender\n",
       "0  00000002_000.png      0           2   80      M\n",
       "1  00000004_000.png      1           4   82      M\n",
       "2  00000005_000.png      0           5   69      F\n",
       "3  00000005_001.png      0           5   69      F\n",
       "4  00000005_002.png      0           5   69      F"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "patient-mapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66692 entries, 0 to 66691\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   image_id    66692 non-null  object\n",
      " 1   label       66692 non-null  int64 \n",
      " 2   patient_id  66692 non-null  int64 \n",
      " 3   age         66692 non-null  int64 \n",
      " 4   gender      66692 non-null  object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interested-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(base_path, image_name) : \n",
    "    image_path = os.path.join(base_path, image_name)\n",
    "    image = cv2.imread(image_path)\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "freelance-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, image_size) : \n",
    "    image = cv2.resize(image, (image_size[0], image_size[1]), interpolation = cv2.INTER_AREA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-transsexual",
   "metadata": {},
   "source": [
    "# Define Config Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "advanced-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"NUM_CLASSES\" : 1,\n",
    "    \"CLASS_NAMES\" : [\"Nodule\"],\n",
    "    \"IMAGE_DATA_DIR\" : \"/media/HHD2/NIH/tflow_obj_detection/images\",\n",
    "    \"METADATA_PATH\" : \"/media/HHD_2TB/baurai/filtered_metadata/metadata.csv\",\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"CHECKPOINT_PATH\" : \"/media/HHD_2TB/baurai/saved_models/chexnet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-water",
   "metadata": {},
   "source": [
    "# Read Images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tamil-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_to_tensor(image_path) : \n",
    "    \"\"\"\n",
    "    input : image path.\n",
    "    output : tensor\n",
    "    \"\"\"\n",
    "    loader = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bibliographic-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_PIL(tensor) : \n",
    "    \"\"\"\n",
    "    input : a tensor.\n",
    "    output : PIL format image\n",
    "    \"\"\"\n",
    "    unloader = transforms.ToPILImage()\n",
    "    \n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "double-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(Dataset) : \n",
    "    def __init__(self, data_dir, image_metadata_file_path, transform = None) : \n",
    "        \"\"\"\n",
    "        data_dir = path to the directory containing images.\n",
    "        image_metadata_file_path = path to the metadata file(csv) containing images and labels.\n",
    "        transform = optional parameter. If specified, the required transforms will be applied on the image.\n",
    "        \"\"\"\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        image_metadata_file = pd.DataFrame(pd.read_csv(image_metadata_file_path))\n",
    "        all_image_ids = image_metadata_file.image_id.values\n",
    "        all_labels = image_metadata_file.label.values\n",
    "        for (image_id, label) in zip(all_image_ids, all_labels) : \n",
    "            full_image_path = os.path.join(data_dir, image_id)\n",
    "            image_paths.append(full_image_path)\n",
    "            labels.append(int(label))\n",
    "        \n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index) : \n",
    "        \"\"\"\n",
    "        Upon passing the index, we get the corresponding image and labels.\n",
    "        \"\"\"\n",
    "        loader = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        image_path = self.image_paths[index]\n",
    "        image = PIL_to_tensor(image_path)\n",
    "        #image = Image.open(image_name).convert(\"RGB\")\n",
    "        #image = loader(image).unsqueeze(0)\n",
    "        label = self.labels[index]\n",
    "        print(label)\n",
    "        if self.transform is not None : \n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label)\n",
    "    \n",
    "    def __len__(self) : \n",
    "        \"\"\"\n",
    "        Returns total number of images available.\n",
    "        \"\"\"\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-remove",
   "metadata": {},
   "source": [
    "## Compute AUC Method : \n",
    "\n",
    "Computes Area Under the Curve (AUC) from prediction scores.\n",
    "    \n",
    "Args:\n",
    "* ground_truth: Pytorch tensor on GPU, shape = [n_samples, n_classes] || true binary labels.\n",
    "* pred: Pytorch tensor on GPU, shape = [n_samples, n_classes] || can either be probability estimates of the positive class, confidence values, or binary decisions.\n",
    "    \n",
    "    \n",
    "It returns the list of AUROCs of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(ground_truth, pred) : \n",
    "    aucrocs = []\n",
    "    ground_truth_numpy = ground_truth.cpu().numpy()\n",
    "    pred_numpu = pred.cpu().numpy()\n",
    "    for i in range(config[\"NUM_CLASSES\"]) : \n",
    "        aucrocs.append(roc_auc_score(ground_truth_numpy[:, i], pred_numpy[:, i]))\n",
    "    return aurocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-bearing",
   "metadata": {},
   "source": [
    "# DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(torch.nn.Module) : \n",
    "    def __init__(self, output_size) : \n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained = True)\n",
    "        num_features = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, output_size),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-sapphire",
   "metadata": {},
   "source": [
    "# Design Model Classes\n",
    "\n",
    "* When performing Normalize, you need to convert to the form of Tensor first.\n",
    "* The operation of Resize and crop is the operation of the format of PIL Image. Now the paper generally resizes the image to (256, 256) and then randomCrop to (224, and 224).\n",
    "* Before transforms.TenCrop(224), you have to add transforms.Scale(256), some of your images might be too small to be cropped to 224.\n",
    "\n",
    "Resize is equivalent to compressing the original image, the approximate shape is not changed, that is, you can see how the image looks.\n",
    "\n",
    "Crop is a random cut of the image, which may be part of the entire image, where RandomCrop is more commonly used.\n",
    "The RandomResizedCrop class is also commonly used. In general, it is to do the crop first, then resize to the specified size.\n",
    "\n",
    "## FiveCrop and TenCrop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spare-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() : \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    model = DenseNet121(config[\"NUM_CLASSES\"]).cuda()\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    \n",
    "    if os.path.isfile(config[\"CHECKPOINT_PATH\"]) : \n",
    "        print(\"===> Loading model from checkpoint\")\n",
    "        checkpoint = torch.load(config[\"CHECKPOINT_PATH\"])\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(\"===> Loaded checkpoint\")\n",
    "    else:\n",
    "        print(\"===> No checkpoint found\")\n",
    "    \n",
    "    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    val_dataset = ChestXRayDataset(data_dir = config[\"IMAGE_DATA_DIR\"], image_metadata_file_path = config[\"METADATA_PATH\"], \n",
    "                                  transform = transforms.Compose([\n",
    "                                      transforms.resize(256),\n",
    "                                      transforms.TenCrop(224),\n",
    "                                      transforms.lambda(lambda crops : \n",
    "                                                        torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                      transforms.lambda(lambda crops : torch.stack([normalize(crop) for crop in crops]))\n",
    "                                  ]))\n",
    "    val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = config[\"BATCH_SIZE\"], shuffle = False,\n",
    "                                            num_workers = 8, pin_memory = True)\n",
    "    \n",
    "    ground_truth = torch.FloatTensor()\n",
    "    ground_truth = ground_truth.cuda()\n",
    "    pred = torch.FloatTensor()\n",
    "    pred = pred.cuda()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (input_image, target) in enumerate(val_loader) : \n",
    "        target = target.cuda()\n",
    "        ground_truth = torch.cat((ground_truth, target), 0)\n",
    "        batch_size, n_crops, channels, height, width = input_image.size()\n",
    "        input_var = torch.autograd.Variable(input_image.view(-1, channels, height, width).cuda(), volatile = True)\n",
    "        output = model(input_var)\n",
    "        output_mean = output.view(batch_size, n_crops, -1).mean(1)\n",
    "        pred = torch.cat((pred, output_mean.data), 0)\n",
    "    \n",
    "    aurocs = compute_AUCs(ground_truth, pred)\n",
    "    aurocs_average = np.array(aurocs).mean()\n",
    "    print(\"Average AUROC = \", aurocs_average)\n",
    "    \n",
    "    for i in range(config[\"NUM_CLASSES\"]) : \n",
    "        print(f\"AUROC of {config[\"CLASS_NAMES\"][i]} is {aurocs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "packed-reunion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\" : \\n    main()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "if __name__ == \"__main__\" : \n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-torture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aditya_pytorch_vm",
   "language": "python",
   "name": "aditya_pytorch_vm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
